#!/usr/bin/env python3
"""
PHASE 9: MANUAL DETECTION GUIDANCE SYSTEM
=========================================
Goal: Build season-aware priority ranking system to guide manual detection efforts
      using 2D probability surfaces (day-of-year √ó time-of-day) enhanced with 
      acoustic indices and environmental data.

Key innovations:
1. 2D probability surfaces from manual detections (kernel density estimation)
2. Cross-station validation (train on 2 stations, test on 1)
3. Acoustic enhancement of baseline probability
4. Environmental feature integration
5. Combined priority ranking system

Use Case: "During active seasons, focus manual detection efforts on these 
          specific time periods first"

This addresses the lessons from Phase 8: work WITH seasonal patterns,
not against them, to provide practical guidance within appropriate seasons.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.neighbors import KernelDensity
from sklearn.metrics import roc_auc_score, average_precision_score
from scipy.stats import pearsonr
from scipy.ndimage import gaussian_filter
import itertools

# Set up plotting style
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

def load_and_prepare_data():
    """Load data and prepare temporal features."""
    
    print("üîÑ PHASE 9: MANUAL DETECTION GUIDANCE SYSTEM")
    print("=" * 75)
    print("Goal: Build 2D probability surfaces to guide manual detection efforts")
    
    # Load aligned dataset
    data_path = Path("data_01_aligned_2021.csv")
    if not data_path.exists():
        raise FileNotFoundError("Run Phase 1 first to create aligned dataset")
    
    df = pd.read_csv(data_path, low_memory=False)
    df['datetime'] = pd.to_datetime(df['datetime'])
    
    # Clean data types
    for col in df.columns:
        if col not in ['datetime', 'station']:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    print(f"‚úì Loaded dataset: {df.shape}")
    
    # Add temporal features
    df['day_of_year'] = df['datetime'].dt.dayofyear
    df['hour_of_day'] = df['datetime'].dt.hour
    df['month'] = df['datetime'].dt.month
    
    # Our data is 2-hour intervals, so we have 12 time periods per day
    # Convert hour to 2-hour period (0-11)
    df['period_of_day'] = df['hour_of_day'] // 2
    
    print(f"‚úì Added temporal features")
    print(f"   Day range: {df['day_of_year'].min()} to {df['day_of_year'].max()}")
    print(f"   Period range: {df['period_of_day'].min()} to {df['period_of_day'].max()}")
    print(f"   Stations: {sorted(df['station'].unique())}")
    
    return df

def identify_target_species(df):
    """Identify species with sufficient data for analysis."""
    
    print(f"\nüéØ IDENTIFYING TARGET SPECIES")
    print("=" * 40)
    
    # Potential target species
    candidate_species = [
        'Spotted seatrout', 'Atlantic croaker', 'Vessel', 
        'Oyster toadfish boat whistle', 'Red drum', 'Silver perch'
    ]
    
    target_species = []
    
    for species in candidate_species:
        if species in df.columns:
            species_data = df[species].fillna(0)
            total_detections = species_data.sum()
            detection_rate = (species_data > 0).mean()
            
            # Minimum thresholds for analysis
            if total_detections >= 100 and detection_rate >= 0.01:
                target_species.append(species)
                print(f"‚úì {species}: {total_detections:.0f} detections ({detection_rate:.1%})")
            else:
                print(f"‚úó {species}: {total_detections:.0f} detections ({detection_rate:.1%}) - insufficient")
    
    print(f"\nüìä Analysis will focus on {len(target_species)} species")
    return target_species

def build_2d_probability_surface(df, species, station_subset, bandwidth=5.0):
    """
    Build 2D probability surface using kernel density estimation.
    
    Args:
        df: DataFrame with detection data
        species: Target species name
        station_subset: List of stations to use for training
        bandwidth: KDE bandwidth parameter
    
    Returns:
        2D array of probabilities (365 days √ó 12 periods)
    """
    
    # Filter to training stations
    train_data = df[df['station'].isin(station_subset)].copy()
    
    # Get detection events
    detections = train_data[train_data[species].fillna(0) > 0]
    
    if len(detections) < 10:  # Minimum detections needed
        print(f"   ‚ö†Ô∏è Only {len(detections)} detections for {species} - returning uniform surface")
        return np.ones((365, 12)) / (365 * 12)
    
    # Extract detection coordinates
    detection_coords = np.column_stack([
        detections['day_of_year'].values,
        detections['period_of_day'].values
    ])
    
    # Create grid for probability surface
    day_grid, period_grid = np.meshgrid(
        np.arange(1, 366),  # Days 1-365
        np.arange(0, 12),   # Periods 0-11
        indexing='ij'
    )
    
    grid_coords = np.column_stack([
        day_grid.ravel(),
        period_grid.ravel()
    ])
    
    # Fit KDE
    kde = KernelDensity(bandwidth=bandwidth, kernel='gaussian')
    kde.fit(detection_coords)
    
    # Calculate log densities and convert to probabilities
    log_densities = kde.score_samples(grid_coords)
    densities = np.exp(log_densities)
    
    # Reshape to 2D surface and normalize
    surface = densities.reshape(365, 12)
    surface = surface / surface.sum()  # Normalize to probabilities
    
    print(f"   ‚úì Built surface from {len(detections)} detections (bandwidth={bandwidth})")
    return surface

def identify_acoustic_features(df, species, method='correlation'):
    """
    Identify relevant acoustic features for each species.
    
    Args:
        df: DataFrame with all data
        species: Target species name
        method: 'correlation', 'pca', or 'top_indices'
    
    Returns:
        List of selected acoustic feature names
    """
    
    # Load species-specific acoustic signatures from Phase 7 if available
    correlation_path = Path("visual_pattern_correlation_check.csv")
    if correlation_path.exists():
        corr_results = pd.read_csv(correlation_path)
        # Use top correlated indices (this was computed for spotted seatrout mainly)
        top_indices = corr_results.head(10)['acoustic_index'].tolist()
        print(f"   Using top correlated indices from Phase 6/7")
        return [idx for idx in top_indices if idx in df.columns]
    
    # Fallback: identify acoustic columns
    exclude_cols = [
        'datetime', 'station', 'day_of_year', 'hour_of_day', 'month', 'period_of_day',
        'Water temp (¬∞C)', 'Water depth (m)', 'Low (50-1200 Hz)', 'High (7000-40000 Hz)', 
        'Broadband (1-40000 Hz)', 'Spotted seatrout', 'Atlantic croaker', 'Vessel',
        'Oyster toadfish boat whistle', 'Red drum', 'Silver perch', 'Black drum',
        'Hardhead catfish', 'total_fish_activity', 'any_activity', 'high_activity', 
        'num_active_species'
    ]
    
    acoustic_cols = [col for col in df.columns if col not in exclude_cols]
    
    if method == 'correlation':
        # Use correlation-based selection
        station_df = df[df['station'] == '9M'].copy()  # Use one station for feature selection
        species_data = station_df[species].fillna(0)
        
        correlations = []
        for acoustic in acoustic_cols:
            if station_df[acoustic].dtype in ['float64', 'int64']:
                acoustic_data = station_df[acoustic].fillna(station_df[acoustic].mean())
                if acoustic_data.std() > 0:
                    r, p = pearsonr(acoustic_data, species_data)
                    if abs(r) > 0.1:  # Lower threshold for broader selection
                        correlations.append((acoustic, abs(r)))
        
        # Sort by correlation strength and take top 10
        correlations.sort(key=lambda x: x[1], reverse=True)
        selected = [x[0] for x in correlations[:10]]
        
    elif method == 'top_indices':
        # Use commonly important acoustic indices
        important_indices = ['ADI', 'ACI', 'NDSI', 'BGNf', 'ENRf', 'Hf', 'LEQf', 'rBA', 'RAOQ']
        selected = [idx for idx in important_indices if idx in acoustic_cols]
    
    else:  # Default to top indices
        selected = acoustic_cols[:10]
    
    print(f"   Selected {len(selected)} acoustic features: {selected[:5]}...")
    return selected

def enhance_surface_with_features(baseline_surface, df, species, station_subset, 
                                acoustic_features, env_features):
    """
    Enhance baseline probability surface with acoustic and environmental features.
    
    Args:
        baseline_surface: 2D baseline probability array
        df: Full DataFrame
        species: Target species name
        station_subset: Training stations
        acoustic_features: List of acoustic feature names
        env_features: List of environmental feature names
    
    Returns:
        Enhanced 2D probability surface
    """
    
    # Filter to training data
    train_data = df[df['station'].isin(station_subset)].copy()
    
    # Create feature matrix
    feature_cols = acoustic_features + env_features + ['day_of_year', 'period_of_day']
    feature_cols = [col for col in feature_cols if col in train_data.columns]
    
    if len(feature_cols) < 2:
        print("   ‚ö†Ô∏è Insufficient features - returning baseline surface")
        return baseline_surface
    
    X = train_data[feature_cols].fillna(train_data[feature_cols].mean())
    y = (train_data[species].fillna(0) > 0).astype(int)
    
    # Simple linear enhancement: compute feature effects
    enhancement_grid = np.ones_like(baseline_surface)
    
    try:
        # For each grid point, estimate enhancement factor
        for day in range(1, 366):
            for period in range(12):
                # Find nearby data points
                day_mask = (train_data['day_of_year'] >= day - 7) & (train_data['day_of_year'] <= day + 7)
                period_mask = (train_data['period_of_day'] == period)
                nearby_mask = day_mask & period_mask
                
                if nearby_mask.sum() > 5:  # Need minimum data points
                    nearby_data = train_data[nearby_mask]
                    nearby_detections = nearby_data[species].fillna(0).sum()
                    nearby_total = len(nearby_data)
                    
                    if nearby_total > 0:
                        local_rate = nearby_detections / nearby_total
                        global_rate = y.mean()
                        
                        if global_rate > 0:
                            enhancement_factor = local_rate / global_rate
                            enhancement_grid[day-1, period] = min(enhancement_factor, 5.0)  # Cap enhancement
        
        # Apply Gaussian smoothing to enhancement grid
        enhancement_grid = gaussian_filter(enhancement_grid, sigma=2.0)
        
        # Combine baseline and enhancement
        enhanced_surface = baseline_surface * enhancement_grid
        enhanced_surface = enhanced_surface / enhanced_surface.sum()  # Renormalize
        
        print("   ‚úì Enhanced surface with feature-based modifiers")
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è Enhancement failed: {e} - using baseline surface")
        enhanced_surface = baseline_surface
    
    return enhanced_surface

def cross_station_validation(df, species, acoustic_features, env_features):
    """
    Perform 3-fold cross-station validation.
    
    Args:
        df: Full DataFrame
        species: Target species name
        acoustic_features: List of acoustic feature names
        env_features: List of environmental feature names
    
    Returns:
        Dict with validation results
    """
    
    print(f"\nüîç CROSS-STATION VALIDATION FOR {species}")
    print("=" * 60)
    
    stations = ['9M', '14M', '37M']
    results = {}
    
    for test_station in stations:
        train_stations = [s for s in stations if s != test_station]
        
        print(f"\nüìä Training on {train_stations}, testing on {test_station}")
        
        # Build baseline surface on training stations
        baseline_surface = build_2d_probability_surface(df, species, train_stations)
        
        # Enhance surface with features
        enhanced_surface = enhance_surface_with_features(
            baseline_surface, df, species, train_stations, acoustic_features, env_features
        )
        
        # Test on held-out station
        test_data = df[df['station'] == test_station].copy()
        test_detections = test_data[species].fillna(0) > 0
        
        if test_detections.sum() < 5:
            print(f"   ‚ö†Ô∏è Only {test_detections.sum()} detections in test data - skipping")
            continue
        
        # Calculate priority scores for test data
        priority_scores = []
        actual_detections = []
        
        for _, row in test_data.iterrows():
            day_idx = int(row['day_of_year']) - 1  # Convert to 0-based index
            period_idx = int(row['period_of_day'])
            
            if 0 <= day_idx < 365 and 0 <= period_idx < 12:
                baseline_score = baseline_surface[day_idx, period_idx]
                enhanced_score = enhanced_surface[day_idx, period_idx]
                priority_scores.append(enhanced_score)
                actual_detections.append(int(row[species] > 0 if not pd.isna(row[species]) else 0))
        
        if len(priority_scores) == 0:
            print("   ‚ö†Ô∏è No valid scores calculated")
            continue
        
        # Calculate validation metrics
        priority_scores = np.array(priority_scores)
        actual_detections = np.array(actual_detections)
        
        # Rank periods by priority score
        ranked_indices = np.argsort(priority_scores)[::-1]  # High to low
        
        # Calculate detection efficiency at different thresholds
        thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]
        efficiency_results = {}
        
        for threshold in thresholds:
            n_check = int(len(ranked_indices) * threshold)
            if n_check > 0:
                top_periods = ranked_indices[:n_check]
                detections_found = actual_detections[top_periods].sum()
                total_detections = actual_detections.sum()
                
                efficiency = detections_found / total_detections if total_detections > 0 else 0
                time_savings = 1 - threshold  # Fraction of time saved
                
                efficiency_results[threshold] = {
                    'detection_efficiency': efficiency,
                    'time_savings': time_savings,
                    'detections_found': detections_found,
                    'total_detections': total_detections
                }
        
        # AUC score if we have both classes
        if len(np.unique(actual_detections)) > 1:
            auc_score = roc_auc_score(actual_detections, priority_scores)
            ap_score = average_precision_score(actual_detections, priority_scores)
        else:
            auc_score = np.nan
            ap_score = np.nan
        
        results[test_station] = {
            'efficiency_results': efficiency_results,
            'auc_score': auc_score,
            'ap_score': ap_score,
            'total_detections': actual_detections.sum(),
            'total_periods': len(actual_detections)
        }
        
        print(f"   Total detections: {actual_detections.sum():,}")
        print(f"   AUC: {auc_score:.3f}, AP: {ap_score:.3f}")
        print(f"   Top 20% efficiency: {efficiency_results.get(0.2, {}).get('detection_efficiency', 0):.1%}")
    
    return results

def create_figure_10_time_savings(species_results, figs_dir):
    """
    FIGURE 10: Create before/after visualization showing traditional vs guided monitoring approach.
    """
    print(f"\nüé® CREATING FIGURE 10: Time Savings Analysis")
    print("=" * 50)
    
    # Calculate time savings data
    species_data = []
    
    for species, results in species_results.items():
        if 'validation_results' in results:
            # Get average detection efficiency and time savings across stations
            detection_effs = []
            time_savings = []
            total_periods = []
            
            for station_result in results['validation_results'].values():
                if 'efficiency_results' in station_result:
                    # Get 20% threshold results
                    eff_20 = station_result['efficiency_results'].get(0.2, {}).get('detection_efficiency', 0)
                    savings_20 = station_result['efficiency_results'].get(0.2, {}).get('time_savings', 0)
                    periods = station_result.get('total_periods', 0)
                    
                    detection_effs.append(eff_20)
                    time_savings.append(savings_20)
                    total_periods.append(periods)
            
            if detection_effs:
                avg_detection_eff = np.mean(detection_effs)
                avg_time_savings = np.mean(time_savings)
                avg_periods = np.mean(total_periods)
                
                species_data.append({
                    'species': species,
                    'detection_efficiency': avg_detection_eff,
                    'time_savings': avg_time_savings,
                    'periods_to_check_traditional': avg_periods,
                    'periods_to_check_guided': avg_periods * (1 - avg_time_savings),
                    'hours_traditional': avg_periods * 5 / 60,  # 5 min per period
                    'hours_guided': avg_periods * (1 - avg_time_savings) * 5 / 60
                })
    
    if not species_data:
        print("‚ö†Ô∏è No data available for time savings analysis")
        return
    
    # Create the visualization
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # Plot 1: Traditional vs Guided Monitoring Effort
    species_names = [d['species'].replace(' ', '\n') for d in species_data]
    traditional_hours = [d['hours_traditional'] for d in species_data]
    guided_hours = [d['hours_guided'] for d in species_data]
    
    x = np.arange(len(species_names))
    width = 0.35
    
    bars1 = ax1.bar(x - width/2, traditional_hours, width, label='Traditional Approach', 
                   color='#ff7f7f', alpha=0.8)
    bars2 = ax1.bar(x + width/2, guided_hours, width, label='Guided Approach', 
                   color='#7fbf7f', alpha=0.8)
    
    ax1.set_xlabel('Species')
    ax1.set_ylabel('Expert Time Required (hours)')
    ax1.set_title('Monitoring Effort Comparison\n(3 months of data)')
    ax1.set_xticks(x)
    ax1.set_xticklabels(species_names)
    ax1.legend()
    ax1.grid(True, alpha=0.3, axis='y')
    
    # Add value labels on bars
    for bar in bars1:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                f'{height:.0f}h', ha='center', va='bottom', fontsize=9)
    
    for bar in bars2:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                f'{height:.0f}h', ha='center', va='bottom', fontsize=9)
    
    # Plot 2: Detection Efficiency vs Time Savings
    detection_effs = [d['detection_efficiency'] * 100 for d in species_data]
    time_savings_pct = [d['time_savings'] * 100 for d in species_data]
    
    scatter = ax2.scatter(time_savings_pct, detection_effs, 
                         s=100, alpha=0.7, c=range(len(species_data)), cmap='viridis')
    
    # Add species labels
    for i, d in enumerate(species_data):
        ax2.annotate(d['species'][:12] + ('...' if len(d['species']) > 12 else ''), 
                    (time_savings_pct[i], detection_effs[i]),
                    xytext=(5, 5), textcoords='offset points', fontsize=9, alpha=0.8)
    
    ax2.set_xlabel('Time Savings (%)')
    ax2.set_ylabel('Detection Efficiency (%)')
    ax2.set_title('Efficiency vs Time Savings Trade-off\n(Top 20% effort threshold)')
    ax2.grid(True, alpha=0.3)
    
    # Add reference lines
    ax2.axhline(y=70, color='red', linestyle='--', alpha=0.5, label='70% efficiency target')
    ax2.axvline(x=80, color='red', linestyle='--', alpha=0.5, label='80% time savings target')
    ax2.legend()
    
    plt.suptitle('Figure 10: Time Savings Analysis - Traditional vs Guided Monitoring', 
                fontsize=16, fontweight='bold', y=1.02)
    plt.tight_layout()
    
    # Save figure
    fig_path = figs_dir / "time_savings_analysis.png"
    plt.savefig(fig_path, dpi=300, bbox_inches='tight')
    print(f"‚úÖ Saved Figure 10: {fig_path}")
    plt.close()
    
    # Also create a summary impact visualization
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))
    
    # Calculate total impact across all species
    total_traditional = sum(traditional_hours)
    total_guided = sum(guided_hours)
    total_savings = total_traditional - total_guided
    avg_detection_eff = np.mean([d['detection_efficiency'] for d in species_data])
    
    # Create summary bars
    categories = ['Total Expert\nTime Required']
    traditional_vals = [total_traditional]
    guided_vals = [total_guided]
    
    x = np.arange(len(categories))
    width = 0.35
    
    bars1 = ax.bar(x - width/2, traditional_vals, width, label='Traditional Approach', 
                  color='#ff7f7f', alpha=0.8)
    bars2 = ax.bar(x + width/2, guided_vals, width, label='Guided Approach', 
                  color='#7fbf7f', alpha=0.8)
    
    ax.set_ylabel('Hours')
    ax.set_title(f'Overall Impact: {total_savings:.0f} Hours Saved ({total_savings/total_traditional:.0%} reduction)\n'
                f'Average Detection Efficiency: {avg_detection_eff:.1%}')
    ax.set_xticks(x)
    ax.set_xticklabels(categories)
    ax.legend()
    ax.grid(True, alpha=0.3, axis='y')
    
    # Add value labels
    for bar, val in zip(bars1, traditional_vals):
        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + bar.get_height()*0.01,
               f'{val:.0f}h', ha='center', va='bottom', fontweight='bold')
    
    for bar, val in zip(bars2, guided_vals):
        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + bar.get_height()*0.01,
               f'{val:.0f}h', ha='center', va='bottom', fontweight='bold')
    
    # Add savings annotation
    ax.annotate(f'SAVINGS:\n{total_savings:.0f} hours', xy=(0, max(traditional_vals)/2), 
               xytext=(0.5, max(traditional_vals)*0.8),
               ha='center', va='center', fontsize=14, fontweight='bold',
               bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),
               arrowprops=dict(arrowstyle='->', color='black', lw=2))
    
    plt.tight_layout()
    
    # Save summary figure
    summary_fig_path = figs_dir / "time_savings_summary.png"
    plt.savefig(summary_fig_path, dpi=300, bbox_inches='tight')
    print(f"‚úÖ Saved time savings summary: {summary_fig_path}")
    plt.close()
    
    return fig_path, summary_fig_path

def generate_visualizations(df, species_results, output_dir):
    """Generate visualization plots for the analysis."""
    
    print(f"\nüìä GENERATING VISUALIZATIONS")
    print("=" * 40)
    
    figs_dir = output_dir / "figures"
    figs_dir.mkdir(exist_ok=True)
    
    # 1. Species activity patterns (seasonal and daily)
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('Species Activity Patterns', fontsize=16, fontweight='bold')
    
    for idx, (species, results) in enumerate(species_results.items()):
        if idx >= 4:  # Limit to 4 species for visualization
            break
        
        row, col = idx // 2, idx % 2
        ax = axes[row, col]
        
        # Plot 2D probability surface for this species (using first available surface)
        if 'surfaces' in results and len(results['surfaces']) > 0:
            surface = list(results['surfaces'].values())[0]['enhanced']
            
            im = ax.imshow(surface.T, aspect='auto', origin='lower', 
                          extent=[1, 365, 0, 11], cmap='YlOrRd')
            ax.set_xlabel('Day of Year')
            ax.set_ylabel('2-Hour Period (0=midnight-2am)')
            ax.set_title(f'{species}')
            plt.colorbar(im, ax=ax, label='Detection Probability')
    
    # Hide unused subplots
    for idx in range(len(species_results), 4):
        row, col = idx // 2, idx % 2
        axes[row, col].set_visible(False)
    
    plt.tight_layout()
    plt.savefig(figs_dir / 'species_probability_surfaces.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. Cross-station validation results
    if species_results:
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # Detection efficiency plot
        species_names = []
        efficiencies_20 = []
        efficiencies_30 = []
        auc_scores = []
        
        for species, results in species_results.items():
            if 'validation_results' in results:
                species_names.append(species.replace(' ', '\n'))
                
                # Get average efficiency across stations
                station_effs_20 = []
                station_effs_30 = []
                station_aucs = []
                
                for station_result in results['validation_results'].values():
                    if 'efficiency_results' in station_result:
                        eff_20 = station_result['efficiency_results'].get(0.2, {}).get('detection_efficiency', 0)
                        eff_30 = station_result['efficiency_results'].get(0.3, {}).get('detection_efficiency', 0)
                        station_effs_20.append(eff_20)
                        station_effs_30.append(eff_30)
                    
                    if not np.isnan(station_result.get('auc_score', np.nan)):
                        station_aucs.append(station_result['auc_score'])
                
                efficiencies_20.append(np.mean(station_effs_20) if station_effs_20 else 0)
                efficiencies_30.append(np.mean(station_effs_30) if station_effs_30 else 0)
                auc_scores.append(np.mean(station_aucs) if station_aucs else 0)
        
        # Plot detection efficiency
        x = np.arange(len(species_names))
        width = 0.35
        
        axes[0].bar(x - width/2, [e*100 for e in efficiencies_20], width, 
                   label='Top 20% periods', alpha=0.8)
        axes[0].bar(x + width/2, [e*100 for e in efficiencies_30], width, 
                   label='Top 30% periods', alpha=0.8)
        
        axes[0].set_xlabel('Species')
        axes[0].set_ylabel('Detection Efficiency (%)')
        axes[0].set_title('Cross-Station Validation: Detection Efficiency')
        axes[0].set_xticks(x)
        axes[0].set_xticklabels(species_names)
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # Plot AUC scores
        axes[1].bar(range(len(species_names)), auc_scores, alpha=0.8, color='green')
        axes[1].set_xlabel('Species')
        axes[1].set_ylabel('AUC Score')
        axes[1].set_title('Cross-Station Validation: AUC Scores')
        axes[1].set_xticks(range(len(species_names)))
        axes[1].set_xticklabels(species_names)
        axes[1].grid(True, alpha=0.3)
        axes[1].axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random baseline')
        axes[1].legend()
        
        plt.tight_layout()
        plt.savefig(figs_dir / 'cross_station_validation.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    # 3. FIGURE 10: Time Savings Analysis
    create_figure_10_time_savings(species_results, figs_dir)
    
    print(f"‚úì Saved visualizations to {figs_dir}")

def create_summary_tables(species_results, output_dir):
    """Create summary tables for results."""
    
    print(f"\nüìã CREATING SUMMARY TABLES")
    print("=" * 40)
    
    tables_dir = output_dir / "tables"
    tables_dir.mkdir(exist_ok=True)
    
    # 1. Species overview table
    overview_data = []
    for species, results in species_results.items():
        if 'validation_results' in results:
            # Aggregate across stations
            total_detections = sum([r.get('total_detections', 0) for r in results['validation_results'].values()])
            avg_auc = np.nanmean([r.get('auc_score', np.nan) for r in results['validation_results'].values()])
            
            # Average efficiency at 20% threshold
            effs_20 = []
            for station_result in results['validation_results'].values():
                eff = station_result.get('efficiency_results', {}).get(0.2, {}).get('detection_efficiency', np.nan)
                if not np.isnan(eff):
                    effs_20.append(eff)
            
            avg_eff_20 = np.mean(effs_20) if effs_20 else np.nan
            
            overview_data.append({
                'Species': species,
                'Total_Detections': total_detections,
                'Avg_AUC': avg_auc,
                'Avg_Detection_Efficiency_20pct': avg_eff_20,
                'Time_Savings_20pct': 0.8 if not np.isnan(avg_eff_20) else np.nan,
                'Stations_Tested': len(results['validation_results'])
            })
    
    overview_df = pd.DataFrame(overview_data)
    overview_df.to_csv(tables_dir / 'species_overview.csv', index=False)
    
    # 2. Detailed validation results table
    detailed_data = []
    for species, results in species_results.items():
        if 'validation_results' in results:
            for station, station_result in results['validation_results'].items():
                for threshold, threshold_result in station_result.get('efficiency_results', {}).items():
                    detailed_data.append({
                        'Species': species,
                        'Test_Station': station,
                        'Threshold': threshold,
                        'Detection_Efficiency': threshold_result.get('detection_efficiency', np.nan),
                        'Time_Savings': threshold_result.get('time_savings', np.nan),
                        'Detections_Found': threshold_result.get('detections_found', np.nan),
                        'Total_Detections': threshold_result.get('total_detections', np.nan),
                        'AUC': station_result.get('auc_score', np.nan),
                        'AP': station_result.get('ap_score', np.nan)
                    })
    
    detailed_df = pd.DataFrame(detailed_data)
    detailed_df.to_csv(tables_dir / 'detailed_validation_results.csv', index=False)
    
    print(f"‚úì Saved summary tables to {tables_dir}")
    
    return overview_df, detailed_df

def main():
    """Main analysis pipeline."""
    
    # Setup output directory
    output_dir = Path("output/phase9_detection_guidance")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Load and prepare data
    df = load_and_prepare_data()
    
    # Identify target species
    target_species = identify_target_species(df)
    
    if not target_species:
        print("‚ùå No species with sufficient data found")
        return
    
    # Environmental features
    env_features = ['Water temp (¬∞C)', 'Low (50-1200 Hz)', 'High (7000-40000 Hz)', 'Broadband (1-40000 Hz)']
    env_features = [f for f in env_features if f in df.columns]
    
    # Analysis results storage
    species_results = {}
    stations = ['9M', '14M', '37M']
    
    # Process each species
    for species in target_species:
        print(f"\n{'='*20} ANALYZING {species.upper()} {'='*20}")
        
        # Identify acoustic features for this species
        acoustic_features = identify_acoustic_features(df, species)
        
        if len(acoustic_features) == 0:
            print(f"   ‚ö†Ô∏è No acoustic features found for {species}")
            continue
        
        print(f"   Using {len(acoustic_features)} acoustic + {len(env_features)} environmental features")
        
        # Build surfaces for each station combination
        surfaces = {}
        for test_station in stations:
            train_stations = [s for s in stations if s != test_station]
            
            # Build baseline surface
            baseline = build_2d_probability_surface(df, species, train_stations)
            
            # Enhance with features
            enhanced = enhance_surface_with_features(
                baseline, df, species, train_stations, acoustic_features, env_features
            )
            
            surfaces[f"train_{'+'.join(train_stations)}_test_{test_station}"] = {
                'baseline': baseline,
                'enhanced': enhanced,
                'train_stations': train_stations,
                'test_station': test_station
            }
        
        # Perform cross-station validation
        validation_results = cross_station_validation(df, species, acoustic_features, env_features)
        
        # Store results
        species_results[species] = {
            'acoustic_features': acoustic_features,
            'env_features': env_features,
            'surfaces': surfaces,
            'validation_results': validation_results
        }
    
    # Generate visualizations
    generate_visualizations(df, species_results, output_dir)
    
    # Create summary tables
    overview_df, detailed_df = create_summary_tables(species_results, output_dir)
    
    # Final summary
    print(f"\nüéâ PHASE 9 COMPLETE!")
    print("=" * 50)
    print(f"‚úÖ Analyzed {len(species_results)} species")
    print(f"‚úÖ Generated 2D probability surfaces")
    print(f"‚úÖ Performed 3-fold cross-station validation")
    print(f"‚úÖ Created visualizations and summary tables")
    
    if not overview_df.empty:
        print(f"\nüìä SUMMARY RESULTS:")
        print(f"   Best performing species (by avg AUC):")
        best_species = overview_df.loc[overview_df['Avg_AUC'].idxmax()]
        print(f"      {best_species['Species']}: AUC={best_species['Avg_AUC']:.3f}, "
              f"20% efficiency={best_species['Avg_Detection_Efficiency_20pct']:.1%}")
        
        print(f"\n   Most efficient species (by detection efficiency):")
        efficient_species = overview_df.loc[overview_df['Avg_Detection_Efficiency_20pct'].idxmax()]
        print(f"      {efficient_species['Species']}: "
              f"20% efficiency={efficient_species['Avg_Detection_Efficiency_20pct']:.1%}, "
              f"AUC={efficient_species['Avg_AUC']:.3f}")
    
    print(f"\nüíæ RESULTS SAVED TO:")
    print(f"   üìä Tables: {output_dir}/tables/")
    print(f"   üìà Figures: {output_dir}/figures/")
    
    print(f"\nüéØ NEXT STEP: Review Quarto report for detailed analysis")
    
    return species_results

if __name__ == "__main__":
    results = main()