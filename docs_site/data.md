# Data Structure

## Overview

Data flows from raw Excel files → processed JSON → web dashboard and Python analysis.

```
data/                      # Raw data (gitignored)
├── 2018/                  # Detection & environmental files
├── 2021/                  # Detection & environmental files  
├── indices/               # Acoustic indices CSV
└── cdn/processed/         # Dashboard JSON files
```

## Raw Data Files

### Species Detection
```
Master_Manual_[STATION]_2h_[YEAR].xlsx
```
- **Stations**: 9M, 14M, 37M
- **Years**: 2018, 2021
- **Content**: Manual species annotations (28 species)
- **Resolution**: 2-hour detection windows

### Environmental Data
```
Master_[STATION]_Temp_[YEAR].xlsx
Master_[STATION]_Depth_[YEAR].xlsx
```
- **Measurements**: Temperature and depth
- **Resolution**: Hourly measurements

### Acoustic Indices
```
AcousticIndices_9M_FullBW_v1.csv
```
- **Content**: 56 acoustic indices per hour
- **Categories**: Temporal, frequency, complexity, diversity, bioacoustic, spectral

## Processed Data

Generated by `npm run process-data`:

```json
detections.json          // 26,280 species detection records
environmental.json       // 237,334 temperature/depth measurements
stations.json           // 3 station definitions
species.json            // 28 species with detection counts  
metadata.json           // Processing summary
```

## Processing Pipeline

```bash
# 1. Process Excel files
npm run process-data

# 2. Validate output
npm run validate-data

# 3. View summary
npm run data-stats
```

## Data Access

### Dashboard (JavaScript)
```javascript
// Data loaded via hooks
import { useCoreData } from '@/lib/hooks/useData';

const { detections, environmental } = useCoreData();
```

### Analysis (Python)
```python
import pandas as pd
import json

# Load processed data
with open('data/cdn/processed/detections.json') as f:
    detections = pd.DataFrame(json.load(f))
```

## Key Statistics

- **Detection Records**: 26,280 manual annotations
- **Environmental Records**: 237,334 temperature/depth measurements
- **Stations**: 3 (9M, 14M, 37M)
- **Species**: 28 tracked species
- **Years**: 2018, 2021
- **Acoustic Indices**: 56 per hour (integration in progress)

## Column Mapping

Species codes mapped via `data/cdn/raw-data/det_column_names.csv`:
- `sp` → Silver perch
- `otbw` → Oyster toadfish boat whistle
- `bde` → Bottlenose dolphin echolocation

## CDN Data Management

### Overview: Dual-Upload System

The project uses Cloudflare R2 CDN with two types of data:

1. **Raw Data Files** (Excel, CSV) - Source files from instruments and manual annotations
2. **Processed JSON Files** - Dashboard-ready data optimized for web visualization

**Why separate?** Raw files are large and change infrequently, while processed files are smaller and regenerated often during development. This minimizes upload time and bandwidth.

### CDN File Structure
```
CDN (Cloudflare R2):
├── raw-data/                          # Raw source files
│   ├── data_manifest.json            # File discovery catalog
│   ├── 2018/                         # Excel files by year
│   ├── 2021/
│   ├── indices/                      # Acoustic indices CSV files
│   └── det_column_names.csv          # Species mappings
│
└── [root]/                           # Processed dashboard files
    ├── detections.json               # Ready for web visualization
    ├── environmental.json
    ├── acoustic_indices.json
    └── metadata.json
```

### The Data Manifest

The **data manifest** (`data_manifest.json`) is a catalog that tells the sync system what files are available on the CDN. Think of it as a "table of contents" for your data files with file sizes, timestamps, and checksums.

**Why needed?** Enables smart downloading - only getting files that are missing or updated, without guessing file names.

### Common Workflows

#### Getting New Raw Data Files

1. **Add files locally**: Place new Excel/CSV files in `data/cdn/raw-data/`

2. **Generate manifest**: 
   ```bash
   npm run generate-manifest
   ```

3. **Upload to CDN**:
   - Upload `data/cdn/raw-data/` folder to CDN
   - Upload the `data_manifest.json` file to CDN root

4. **Process locally**:
   ```bash
   npm run process-data
   ```

5. **Upload processed data**: Upload files from `public/data/` to CDN root

#### Setting Up on New Computer

1. **Get latest data**:
   ```bash
   npm run sync-data              # Download new/changed files
   npm run sync-data:check        # Check what needs updating
   ```

2. **Process if needed**:
   ```bash
   uv run scripts/utils/check_data_freshness.py    # Check if processing needed
   npm run process-data                             # If needed
   ```

3. **Start development**:
   ```bash
   npm run dev
   ```

### Smart Commands

```bash
# DATA DISCOVERY & SYNC
npm run sync-data:check          # Check what files need updating (no downloads)
npm run sync-data                # Download all new/changed files
npm run sync-data:force          # Download everything (ignore local versions)
npm run sync-data:indices        # Just download acoustic indices files

# DATA PREPARATION  
npm run generate-manifest        # Create catalog of local files
npm run process-data            # Convert Excel files to web format
uv run scripts/utils/check_data_freshness.py  # Check if reprocessing needed

# DEVELOPMENT
npm run dev                     # Start dashboard (uses existing data)
npm run dev:fresh              # Process data then start dashboard
```

### Quick Reference

| Situation | Commands |
|-----------|----------|
| **First time setup** | `npm run sync-data` → `npm run process-data` → `npm run dev` |
| **Daily development** | `npm run dev` |
| **New data from collaborator** | `npm run sync-data` → `npm run process-data` |
| **Added files locally** | `npm run generate-manifest` → upload to CDN |
| **Dashboard looks wrong** | Check with `check_data_freshness.py` → `npm run process-data` |

### File Management Notes

- **Raw data**: Downloaded from CDN using manifest-based sync
- **Processed data**: Generated locally, uploaded to CDN for dashboard
- **Intermediate results**: Saved to `data/intermediate_results/` during analysis
- **Local cache**: Raw files cached locally to avoid repeated downloads